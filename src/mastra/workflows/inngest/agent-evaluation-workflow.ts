/**
 * Agent Evaluation Workflow (Inngest)
 *
 * This workflow is designed to comprehensively evaluate the performance and quality
 * of various AI agents using a defined set of metrics and prompts.
 *
 * @module agent-evaluation-workflow
 * @license Apache-2.0
 * @generated on 2025-06-02
 */

import { z } from 'zod';
import { inngest } from '../../inngest';
import { init } from '@mastra/inngest';
import { generateId } from 'ai';
import { PinoLogger } from '@mastra/loggers';

// Import all agents
import { masterAgent } from '../../agents/masterAgent';
import { supervisorAgent } from '../../agents/supervisorAgent';
import { mcpAgent } from '../../agents/mcpAgent';
import { ragAgent } from '../../agents/ragAgent';
import { stockAgent } from '../../agents/stockAgent';
import { weatherAgent } from '../../agents/weather-agent';
import { workerAgent } from '../../agents/workerAgent';
import { evaluationAgent } from '../../agents/evaluationAgent';
import { Evals } from '../../evals'; // Import Evals type from evals
import { dataManagerAgent } from '../../agents/dataManagerAgent';

// Import observability components
import { promptManager } from '../../observability/promptManager';
import { createTracedGoogleModel, traceAgentOperation } from '../../observability';

const { createWorkflow, createStep } = init(inngest);

const logger = new PinoLogger({
  name: 'agent-evaluation-workflow',
  level: 'info',
});

// Agent Registry - includes all agents available for evaluation
const agentRegistry = {
  masterAgent,
  supervisorAgent,
  mcpAgent,
  ragAgent,
  stockAgent,
  weatherAgent,
  workerAgent,
  evaluationAgent,
  dataManagerAgent,
};

// Input Schema for the Agent Evaluation Workflow
export const evaluationInputSchema = z.object({
  targetAgentName: z.string().describe('The name of the agent to be evaluated (e.g., "Master Agent")'),
  evaluationData: z.array(z.object({
    input: z.string().describe('The input query or data for the agent to process'),
    expectedOutput: z.string().optional().describe('The expected output for the evaluation, if applicable'),
    context: z.record(z.any()).optional().describe('Additional context for the agent during processing'),
    evaluationCriteria: z.record(z.any()).optional().describe('Specific criteria for this evaluation case (e.g., {"accuracy": true})'),
  })).min(1).describe('An array of test cases for evaluation'),
  metricsToApply: z.array(z.string()).describe('A list of metric names from evaluationAgent.evals to apply (e.g., ["toxicityMetric", "fluencyMetric"])').default([]),
  evaluationPurpose: z.string().optional().describe('A brief description of the purpose of this evaluation run'),
  saveResults: z.boolean().default(true).describe('Whether to save the detailed evaluation results to dataManagerAgent'),
});

// Output Schema for the Agent Evaluation Workflow
export const evaluationOutputSchema = z.object({
  evaluationId: z.string().describe('Unique ID for this evaluation run'),
  targetAgentName: z.string().describe('The name of the agent that was evaluated'),
  evaluationResults: z.array(z.object({
    input: z.string().describe('The input query or data used for evaluation'),
    agentOutput: z.string().describe('The actual output produced by the agent'),
    expectedOutput: z.string().optional().describe('The expected output, if provided'),
    metricResults: z.record(z.any()).describe('Results from applied metrics (e.g., {"toxicityScore": 0.1})'),
    overallScore: z.number().optional().describe('An aggregated overall score for this test case'),
    evaluationNotes: z.array(z.string()).describe('Any specific notes or observations during evaluation'),
  })).describe('Detailed results for each evaluation test case'),
  summaryReport: z.string().describe('A summarized report of the evaluation findings'),
  recommendations: z.array(z.string()).describe('Recommendations for agent improvement'),
  evaluationDurationMs: z.number().describe('The total duration of the evaluation run in milliseconds'),
  status: z.enum(['completed', 'failed', 'partial_success']).describe('Overall status of the evaluation'),
});

/**
 * Step 1: Evaluation Planning and Strategy Generation
 * Uses Master Agent to plan the evaluation.
 */
const evaluationPlanningStep = createStep({
  id: 'evaluation-planning',
  inputSchema: evaluationInputSchema,
  outputSchema: z.object({
    targetAgentName: z.string(),
    evaluationData: evaluationInputSchema.shape.evaluationData,
    metricsToApply: evaluationInputSchema.shape.metricsToApply,
    evaluationPurpose: evaluationInputSchema.shape.evaluationPurpose,
    saveResults: evaluationInputSchema.shape.saveResults,
    evaluationPlan: z.string().describe('A plan generated by the Master Agent for the evaluation'),
  }),
  execute: async ({ inputData }) => {
    const { targetAgentName, evaluationData, metricsToApply, evaluationPurpose } = inputData;
    logger.info('Starting evaluation planning', { targetAgentName, evaluationPurpose });

    // Use promptManager to get the evaluation prompt
    const evaluationPromptTemplate = promptManager.getPrompt('evaluation');
    if (!evaluationPromptTemplate) {
      throw new Error('Evaluation prompt template not found in promptManager.');
    }

    const evaluationPlanQuery = promptManager.renderPrompt('evaluation', {
      output: `Agent to evaluate: ${targetAgentName}
Test Cases: ${evaluationData.length}`,
      expected: 'Comprehensive evaluation report',
      context: evaluationPurpose || 'General agent performance evaluation',
      focus: `Metrics to apply: ${metricsToApply.join(', ')}`,
    });

    // Master Agent generates the evaluation plan
    const planningResponse = await traceAgentOperation(
      masterAgent.generate,
      'Master Agent',
      'analyze'
    )([{ role: 'user', content: evaluationPlanQuery }], {});

    const evaluationPlan = planningResponse.text || 'No specific plan generated.';
    logger.info('Evaluation plan generated', { evaluationPlan });

    return {
      targetAgentName,
      evaluationData,
      metricsToApply,
      evaluationPurpose,
      saveResults: inputData.saveResults,
      evaluationPlan,
    };
  },
});

/**
 * Step 2: Agent Execution for each test case
 * The target agent processes each input.
 */
const agentExecutionStep = createStep({
  id: 'agent-execution',
  inputSchema: evaluationPlanningStep.outputSchema,
  outputSchema: z.object({
    targetAgentName: z.string(),
    metricsToApply: evaluationInputSchema.shape.metricsToApply,
    evaluationPurpose: evaluationInputSchema.shape.evaluationPurpose,
    saveResults: evaluationInputSchema.shape.saveResults,
    evaluationPlan: z.string(),
    executionResults: z.array(z.object({
      input: z.string(),
      expectedOutput: z.string().optional(),
      agentOutput: z.string(),
      executionTimeMs: z.number(),
      status: z.enum(['success', 'failure']),
      errorMessage: z.string().optional(),
    })),
  }),
  execute: async ({ inputData }) => {
    const { targetAgentName, evaluationData } = inputData;
    logger.info('Executing agent for evaluation', { targetAgentName, numTestCases: evaluationData.length });

    const targetAgent = agentRegistry[targetAgentName as keyof typeof agentRegistry];
    if (!targetAgent) {
      throw new Error(`Target agent "${targetAgentName}" not found in registry.`);
    }

    const executionResults = [];
    for (const testCase of evaluationData) {
      const startTime = Date.now();
      let agentOutput = '';
      let status: 'success' | 'failure' = 'failure';
      let errorMessage: string | undefined;

      try {
        const response = await traceAgentOperation(
          targetAgent.generate,
          targetAgentName,
          'generate'
        )([{ role: 'user', content: testCase.input }], testCase.context || {});
        agentOutput = response.text || '';
        status = 'success';
      } catch (error) {
        errorMessage = error instanceof Error ? error.message : String(error);
        logger.error(`Agent execution failed for ${targetAgentName} with input "${testCase.input.substring(0, 50)}..."`, { error: errorMessage });
      } finally {
        const executionTimeMs = Date.now() - startTime;
        executionResults.push({
          input: testCase.input,
          expectedOutput: testCase.expectedOutput,
          agentOutput,
          executionTimeMs,
          status,
          errorMessage,
        });
      }
    }
    logger.info('Agent execution completed.', { totalCases: executionResults.length });

    return {
      targetAgentName,
      metricsToApply: inputData.metricsToApply,
      evaluationPurpose: inputData.evaluationPurpose,
      saveResults: inputData.saveResults,
      evaluationPlan: inputData.evaluationPlan,
      executionResults,
    };
  },
});

/**
 * Step 3: Apply Metrics using Evaluation Agent
 * The evaluation agent applies specified metrics to agent outputs.
 */
const metricApplicationStep = createStep({
  id: 'metric-application',
  inputSchema: agentExecutionStep.outputSchema,
  outputSchema: z.object({
    targetAgentName: z.string(),
    evaluationPurpose: z.string().optional(),
    saveResults: z.boolean(),
    evaluationPlan: z.string(),
    detailedMetricResults: z.array(z.object({
      input: z.string(),
      expectedOutput: z.string().optional(),
      agentOutput: z.string(),
      executionTimeMs: z.number(),
      metricResults: z.record(z.any()), // Can be more specific with Evals
      overallScore: z.number().optional(),
      evaluationNotes: z.array(z.string()),
    })),
  }),
  execute: async ({ inputData }) => {
    const { targetAgentName, executionResults, metricsToApply } = inputData;
    logger.info('Applying metrics with Evaluation Agent', { targetAgentName, metrics: metricsToApply });

    const detailedMetricResults = [];

    for (const result of executionResults) {
      const metricResults: Record<string, any> = {};
      const evaluationNotes: string[] = [];
      let totalScore = 0;
      let appliedMetricsCount = 0;

      // Access evals from evaluationAgent
      const availableEvals = evaluationAgent.evals as Evals;

      for (const metricName of metricsToApply) {
        if (metricName in availableEvals && typeof availableEvals[metricName as keyof Evals].measure === 'function') {
          try {
            const metric = availableEvals[metricName as keyof Evals];
            const measureFunction = metric.measure as (input: string, output: string, options?: any) => Promise<any>;

            const score = await measureFunction(
              result.input,
              result.agentOutput,
              { expected: result.expectedOutput } // Pass expected output if applicable
            );
            metricResults[metricName] = score;
            totalScore += typeof score === 'number' ? score : (score?.score || 0); // Assuming metrics return a score property or raw number
            appliedMetricsCount++;
          } catch (metricError) {
            const errorMessage = metricError instanceof Error ? metricError.message : String(metricError);
            metricResults[metricName] = { error: errorMessage, status: 'failed' };
            evaluationNotes.push(`Metric "${metricName}" failed: ${errorMessage}`);
            logger.error(`Metric "${metricName}" failed for input "${result.input.substring(0, 50)}..."`, { error: errorMessage });
          }
        } else {
          evaluationNotes.push(`Metric "${metricName}" not found or not measureable.`);
          logger.warn(`Requested metric "${metricName}" not found or not measureable in evaluationAgent.evals.`);
        }
      }

      const overallScore = appliedMetricsCount > 0 ? totalScore / appliedMetricsCount : undefined;

      detailedMetricResults.push({
        input: result.input,
        expectedOutput: result.expectedOutput,
        agentOutput: result.agentOutput,
        executionTimeMs: result.executionTimeMs,
        metricResults,
        overallScore,
        evaluationNotes,
      });
    }

    logger.info('Metric application completed.', { numResults: detailedMetricResults.length });
    return {
      targetAgentName: inputData.targetAgentName,
      evaluationPurpose: inputData.evaluationPurpose,
      saveResults: inputData.saveResults,
      evaluationPlan: inputData.evaluationPlan,
      detailedMetricResults,
    };
  },
});

/**
 * Step 4: Generate Evaluation Report and Recommendations
 * Uses Supervisor Agent or Data Manager Agent for final reporting.
 */
const reportGenerationStep = createStep({
  id: 'report-generation',
  inputSchema: metricApplicationStep.outputSchema,
  outputSchema: evaluationOutputSchema,
  execute: async ({ inputData }) => {
    const { targetAgentName, detailedMetricResults, evaluationPurpose, evaluationPlan, saveResults } = inputData;
    logger.info('Generating evaluation report', { targetAgentName, evaluationPurpose });

    const evaluationId = generateId();
    let totalOverallScore = 0;
    let successfulCases = 0;
    const allEvaluationNotes: string[] = [];

    detailedMetricResults.forEach(caseResult => {
      if (caseResult.overallScore !== undefined) {
        totalOverallScore += caseResult.overallScore;
        successfulCases++;
      }
      allEvaluationNotes.push(...caseResult.evaluationNotes);
    });

    const averageOverallScore = successfulCases > 0 ? totalOverallScore / successfulCases : 0;

    // Use promptManager to get the evaluation report prompt
    const reportPromptTemplate = promptManager.getPrompt('evaluation-report-generator');
    if (!reportPromptTemplate) {
      throw new Error('Evaluation report generator prompt template not found in promptManager.');
    }

    const detailedResultsString = detailedMetricResults.map((res, idx) => `
        ### Test Case ${idx + 1}
        - Input: "${res.input}"
        - Agent Output: "${res.agentOutput}"
        - Expected Output: "${res.expectedOutput || 'N/A'}"
        - Overall Score: ${res.overallScore !== undefined ? res.overallScore.toFixed(2) : 'N/A'}
        - Metric Results: ${JSON.stringify(res.metricResults, null, 2)}
        - Notes: ${res.evaluationNotes.length > 0 ? res.evaluationNotes.join('; ') : 'None'}
      `).join('\n---\n');

    // Initial content for the supervisor agent to generate a summary and recommendations
    const rawReportContent = `
      Agent Name: ${targetAgentName}
      Total Test Cases: ${detailedMetricResults.length}
      Successful Metric Applications: ${successfulCases}
      Average Overall Score: ${averageOverallScore.toFixed(2)}

      Detailed Results:
      ${detailedResultsString}

      Based on the detailed results, please provide a concise summary and clear recommendations for improving the agent. Format it as: "Summary: ...\nRecommendations: ..."
    `;

    // Use Supervisor Agent to summarize and provide recommendations
    const summaryAndRecommendationsResponse = await traceAgentOperation(
      supervisorAgent.generate,
      'Supervisor Agent',
      'analyze'
    )([{ role: 'user', content: rawReportContent }], {});

    const rawResponseText = summaryAndRecommendationsResponse.text || '';
    const summaryMatch = rawResponseText.match(/Summary: ([\s\S]*?)(?:\nRecommendations:|$)/);
    const recommendationsMatch = rawResponseText.match(/Recommendations: ([\s\S]*)/);

    const summary = summaryMatch ? summaryMatch[1].trim() : 'No summary generated.';
    const recommendations = recommendationsMatch ? recommendationsMatch[1].split('\n').map(s => s.trim()).filter(s => s) : [];

    const reportContent = promptManager.renderPrompt('evaluation-report-generator', {
      agentName: targetAgentName,
      summary: summary,
      detailedResults: detailedResultsString,
      recommendations: recommendations.join('\n- '),
    }).replace('{{DATE}}', new Date().toISOString()); // Replace placeholder for current date

    // Save results using Data Manager Agent if enabled
    if (saveResults) {
      logger.info('Saving evaluation results using Data Manager Agent.');
      // This is a conceptual call; actual dataManagerAgent tool would need to be defined
      // For now, we'll just log that it would be saved.
      try {
        // Assuming dataManagerAgent has a tool to store structured data
        // Example: await dataManagerAgent.callTool('storeEvaluationResults', {
        //   evaluationId,
        //   targetAgentName,
        //   detailedMetricResults,
        //   summaryReport,
        //   recommendations,
        // });
        logger.info('Evaluation results conceptually saved by Data Manager Agent.', { evaluationId });
      } catch (error) {
        logger.error('Failed to save evaluation results with Data Manager Agent.', { error });
        allEvaluationNotes.push(`Failed to save results: ${error instanceof Error ? error.message : String(error)}`);
      }
    }

    let status: 'completed' | 'failed' | 'partial_success';
    if (successfulCases === detailedMetricResults.length) {
      status = 'completed';
    } else if (successfulCases > 0) {
      status = 'partial_success';
    } else {
      status = 'failed';
    }
    
    const evaluationDurationMs = Date.now() - (detailedMetricResults.length > 0 ? detailedMetricResults[0].executionTimeMs : Date.now()); // Simplified duration, consider capturing start time of report generation step more accurately

    return {
      evaluationId,
      targetAgentName,
      evaluationResults: detailedMetricResults,
      summaryReport: summary,
      recommendations,
      evaluationDurationMs,
      status,
    };
  },
});

/**
 * Main Agent Evaluation Workflow
 */
export const agentEvaluationWorkflow = createWorkflow({
  id: 'agent-evaluation-workflow',
  inputSchema: evaluationInputSchema,
  outputSchema: evaluationOutputSchema,
})
.then(evaluationPlanningStep)
.then(agentExecutionStep)
.then(metricApplicationStep)
.then(reportGenerationStep)
.commit(); 